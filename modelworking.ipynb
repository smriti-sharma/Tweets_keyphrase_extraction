{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bi_lstm_model.py\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class Model(object):\n",
    "\n",
    "    def __init__(self,\n",
    "                 nh1,\n",
    "                 nh2,\n",
    "                 ny,\n",
    "                 nz,\n",
    "                 de,\n",
    "                 cs,\n",
    "                 lr,\n",
    "                 lr_decay,\n",
    "                 embedding,\n",
    "                 max_gradient_norm,\n",
    "                 model_cell='rnn',\n",
    "                 model='basic_model',\n",
    "                 nonstatic=False):\n",
    "\n",
    "        self.batch_size = tf.placeholder(dtype=tf.int32, shape=None)\n",
    "        self.input_x=tf.placeholder(tf.int32,shape=[None,None,cs],name='input_x')\n",
    "        self.input_y=tf.placeholder(tf.int32,shape=[None,None],name=\"input_y\")\n",
    "        self.input_z=tf.placeholder(tf.int32,shape=[None,None],name='input_z')\n",
    "        self.keep_prob=tf.placeholder(dtype=tf.float32,name='keep_prob')\n",
    "\n",
    "        self.lr=tf.Variable(lr,dtype=tf.float32)\n",
    "\n",
    "        self.learning_rate_decay_op = self.lr.assign(\n",
    "            self.lr * lr_decay)\n",
    "\n",
    "\n",
    "        #Creating embedding input\n",
    "        with tf.device(\"/cpu:0\"),tf.name_scope('embedding'):\n",
    "            if nonstatic:\n",
    "                W=tf.constant(embedding,name='embW',dtype=tf.float32)\n",
    "            else:\n",
    "                W=tf.Variable(embedding,name='embW',dtype=tf.float32)\n",
    "            inputs=tf.nn.embedding_lookup(W,self.input_x)\n",
    "            inputs=tf.reshape(inputs,[self.batch_size,-1,cs*de])\n",
    "\n",
    "        #Droupout embedding input\n",
    "        inputs=tf.nn.dropout(inputs,keep_prob=self.keep_prob,name='drop_inputs')\n",
    "\n",
    "        #Create the internal multi-layer cell for rnn\n",
    "        if model_cell=='rnn':\n",
    "            single_cell0=tf.nn.rnn_cell.BasicRNNCell(nh1)\n",
    "            single_cell1=tf.nn.rnn_cell.BasicRNNCell(nh1)\n",
    "            single_cell2=tf.nn.rnn_cell.BasicRNNCell(nh2)\n",
    "        elif model_cell=='lstm':\n",
    "            single_cell0=tf.nn.rnn_cell.BasicLSTMCell(nh1,state_is_tuple=True)\n",
    "            single_cell1=tf.nn.rnn_cell.BasicLSTMCell(nh1,state_is_tuple=True)\n",
    "            single_cell2=tf.nn.rnn_cell.BasicLSTMCell(nh2,state_is_tuple=True)\n",
    "        elif model_cell=='gru':\n",
    "            single_cell0=tf.nn.rnn_cell.GRUCell(nh1)\n",
    "            single_cell1=tf.nn.rnn_cell.GRUCell(nh1)\n",
    "            single_cell2=tf.nn.rnn_cell.GRUCell(nh2)\n",
    "        else:\n",
    "            raise 'model_cell error!'\n",
    "        #DropoutWrapper rnn_cell\n",
    "        single_cell0 = tf.nn.rnn_cell.DropoutWrapper(single_cell0,output_keep_prob=self.keep_prob)\n",
    "        single_cell1 = tf.nn.rnn_cell.DropoutWrapper(single_cell1, output_keep_prob=self.keep_prob)\n",
    "        single_cell2 = tf.nn.rnn_cell.DropoutWrapper(single_cell2, output_keep_prob=self.keep_prob)\n",
    "        \n",
    "        self.init_state=single_cell1.zero_state(self.batch_size,dtype=tf.float32) \n",
    "        \n",
    "        \n",
    "        #Bi-RNN1\n",
    "\n",
    "        x_len = tf.cast(tf.shape(inputs)[1], tf.int64)\n",
    "        batch=2\n",
    "        with tf.variable_scope('bi_rnn1'):\n",
    "            self.outputs1,self.state1=tf.nn.bidirectional_dynamic_rnn(\n",
    "                single_cell0,\n",
    "                single_cell1,\n",
    "                inputs,\n",
    "                sequence_length=[x_len]*batch,\n",
    "                dtype=tf.float32\n",
    "            )\n",
    "\n",
    "        self.outputs1=tf.concat(2,self.outputs1)\n",
    "\n",
    "        \n",
    "        #RNN2\n",
    "        with tf.variable_scope('rnn2'):\n",
    "            self.outputs2,self.state2=tf.nn.dynamic_rnn(\n",
    "                cell=single_cell2,\n",
    "                inputs=self.outputs1,\n",
    "                initial_state=self.init_state,\n",
    "                dtype=tf.float32\n",
    "            )\n",
    "\n",
    "        #outputs_y\n",
    "        with tf.variable_scope('output_sy'):\n",
    "            w_y=tf.get_variable(\"softmax_w_y\",[2*nh1,ny])\n",
    "            b_y=tf.get_variable(\"softmax_b_y\",[ny])\n",
    "            outputs1=tf.reshape(self.outputs1,[-1,2*nh1])\n",
    "            sy=tf.nn.xw_plus_b(outputs1,w_y,b_y)\n",
    "            self.sy_pred = tf.reshape(tf.argmax(sy, 1), [self.batch_size, -1])\n",
    "        #outputs_z\n",
    "        with tf.variable_scope('output_sz'):\n",
    "            w_z = tf.get_variable(\"softmax_w_z\", [nh2, nz])\n",
    "            b_z = tf.get_variable(\"softmax_b_z\", [nz])\n",
    "            outputs2 = tf.reshape(self.outputs2, [-1, nh2])\n",
    "            sz = tf.nn.xw_plus_b(outputs2, w_z,b_z)\n",
    "            self.sz_pred = tf.reshape(tf.argmax(sz, 1), [self.batch_size, -1])\n",
    "        #loss\n",
    "        with tf.variable_scope('loss'):\n",
    "            label_y = tf.reshape(self.input_y, [-1])\n",
    "            loss1 = tf.nn.sparse_softmax_cross_entropy_with_logits(sy, label_y)\n",
    "            label_z = tf.reshape(self.input_z, [-1])\n",
    "            loss2 = tf.nn.sparse_softmax_cross_entropy_with_logits(sz, label_z)\n",
    "            self.loss=tf.reduce_sum(0.5*loss1+0.5*loss2)/tf.cast(self.batch_size,tf.float32)\n",
    "\n",
    "        tvars=tf.trainable_variables()\n",
    "        grads,_=tf.clip_by_global_norm(tf.gradients(self.loss,tvars),max_gradient_norm)\n",
    "        optimizer=tf.train.GradientDescentOptimizer(self.lr)\n",
    "        self.train_op=optimizer.apply_gradients(zip(grads,tvars))\n",
    "\n",
    "    def cost(output, target):\n",
    "        # Compute cross entropy for each frame.\n",
    "        cross_entropy = target * tf.log(output)\n",
    "        cross_entropy = -tf.reduce_sum(cross_entropy, reduction_indices=2)\n",
    "        mask = tf.sign(tf.reduce_max(tf.abs(target), reduction_indices=2))\n",
    "        cross_entropy *= mask\n",
    "        # Average over actual sequence lengths.\n",
    "        cross_entropy = tf.reduce_sum(cross_entropy, reduction_indices=1)\n",
    "        cross_entropy /= tf.reduce_sum(mask, reduction_indices=1)\n",
    "        return tf.reduce_mean(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.py\n",
    "\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "class Model(object):\n",
    "\n",
    "    def __init__(self,\n",
    "                 nh1,\n",
    "                 nh2,\n",
    "                 ny,\n",
    "                 nz,\n",
    "                 de,\n",
    "                 cs,\n",
    "                 lr,\n",
    "                 lr_decay,\n",
    "                 embedding,\n",
    "                 max_gradient_norm,\n",
    "                 model_cell='rnn',\n",
    "                 model='basic_model',\n",
    "                 nonstatic=False):\n",
    "\n",
    "        self.batch_size = tf.placeholder(dtype=tf.int32, shape=None)\n",
    "        self.input_x=tf.placeholder(tf.int32,shape=[None,None,cs],name='input_x')\n",
    "        self.input_y=tf.placeholder(tf.int32,shape=[None,None],name=\"input_y\")\n",
    "        self.input_z=tf.placeholder(tf.int32,shape=[None,None],name='input_z')\n",
    "        self.keep_prob=tf.placeholder(dtype=tf.float32,name='keep_prob')\n",
    "\n",
    "        self.lr=tf.Variable(lr,dtype=tf.float32)\n",
    "\n",
    "        self.learning_rate_decay_op = self.lr.assign(\n",
    "            self.lr * lr_decay)\n",
    "\n",
    "\n",
    "        #Creating embedding input\n",
    "        with tf.device(\"/cpu:0\"),tf.name_scope('embedding'):\n",
    "            if nonstatic:\n",
    "                W=tf.constant(embedding,name='embW',dtype=tf.float32)\n",
    "            else:\n",
    "                W=tf.Variable(embedding,name='embW',dtype=tf.float32)\n",
    "            inputs=tf.nn.embedding_lookup(W,self.input_x)\n",
    "            inputs=tf.reshape(inputs,[self.batch_size,-1,cs*de])\n",
    "\n",
    "        #Droupout embedding input\n",
    "        inputs=tf.nn.dropout(inputs,keep_prob=self.keep_prob,name='drop_inputs')\n",
    "\n",
    "        #Create the internal multi-layer cell for rnn\n",
    "        if model_cell=='rnn':\n",
    "            single_cell1=tf.nn.rnn_cell.BasicRNNCell(nh1)\n",
    "            single_cell2=tf.nn.rnn_cell.BasicRNNCell(nh2)\n",
    "        elif model_cell=='lstm':\n",
    "            single_cell1=tf.nn.rnn_cell.BasicLSTMCell(nh1,state_is_tuple=True)\n",
    "            single_cell2=tf.nn.rnn_cell.BasicLSTMCell(nh2,state_is_tuple=True)\n",
    "        elif model_cell=='gru':\n",
    "            single_cell1=tf.nn.rnn_cell.GRUCell(nh1)\n",
    "            single_cell2=tf.nn.rnn_cell.GRUCell(nh2)\n",
    "        else:\n",
    "            raise 'model_cell error!'\n",
    "        #DropoutWrapper rnn_cell\n",
    "        single_cell1 = tf.nn.rnn_cell.DropoutWrapper(single_cell1, output_keep_prob=self.keep_prob)\n",
    "        single_cell2 = tf.nn.rnn_cell.DropoutWrapper(single_cell2, output_keep_prob=self.keep_prob)\n",
    "      \n",
    "        self.init_state=single_cell1.zero_state(self.batch_size,dtype=tf.float32) \n",
    "        \n",
    "        #RNN1\n",
    "        with tf.variable_scope('rnn1'):\n",
    "            self.outputs1,self.state1=tf.nn.dynamic_rnn(\n",
    "                cell=single_cell1,\n",
    "                inputs=inputs,\n",
    "                initial_state=self.init_state,\n",
    "                dtype=tf.float32\n",
    "            )\n",
    "\n",
    "        #RNN2\n",
    "        with tf.variable_scope('rnn2'):\n",
    "            self.outputs2,self.state2=tf.nn.dynamic_rnn(\n",
    "                cell=single_cell2,\n",
    "                inputs=self.outputs1,\n",
    "                initial_state=self.init_state,\n",
    "                dtype=tf.float32\n",
    "            )\n",
    "\n",
    "        #outputs_y\n",
    "        with tf.variable_scope('output_sy'):\n",
    "            w_y=tf.get_variable(\"softmax_w_y\",[nh1,ny])\n",
    "            b_y=tf.get_variable(\"softmax_b_y\",[ny])\n",
    "            outputs1=tf.reshape(self.outputs1,[-1,nh1])\n",
    "            sy=tf.nn.xw_plus_b(outputs1,w_y,b_y)\n",
    "            self.sy_pred = tf.reshape(tf.argmax(sy, 1), [self.batch_size, -1])\n",
    "        #outputs_z\n",
    "        with tf.variable_scope('output_sz'):\n",
    "            w_z = tf.get_variable(\"softmax_w_z\", [nh2, nz])\n",
    "            b_z = tf.get_variable(\"softmax_b_z\", [nz])\n",
    "            outputs2 = tf.reshape(self.outputs2, [-1, nh2])\n",
    "            sz = tf.nn.xw_plus_b(outputs2, w_z,b_z)\n",
    "            self.sz_pred = tf.reshape(tf.argmax(sz, 1), [self.batch_size, -1])\n",
    "        #loss\n",
    "        with tf.variable_scope('loss'):\n",
    "            label_y = tf.reshape(self.input_y, [-1])\n",
    "            loss1 = tf.nn.sparse_softmax_cross_entropy_with_logits(sy, label_y)\n",
    "            label_z = tf.reshape(self.input_z, [-1])\n",
    "            loss2 = tf.nn.sparse_softmax_cross_entropy_with_logits(sz, label_z)\n",
    "            self.loss=tf.reduce_sum(0.5*loss1+0.5*loss2)/tf.cast(self.batch_size,tf.float32)\n",
    "\n",
    "        tvars=tf.trainable_variables()\n",
    "        grads,_=tf.clip_by_global_norm(tf.gradients(self.loss,tvars),max_gradient_norm)\n",
    "        optimizer=tf.train.GradientDescentOptimizer(self.lr)\n",
    "        self.train_op=optimizer.apply_gradients(zip(grads,tvars))\n",
    "\n",
    "    def cost(output, target):\n",
    "        # Compute cross entropy for each frame.\n",
    "        cross_entropy = target * tf.log(output)\n",
    "        cross_entropy = -tf.reduce_sum(cross_entropy, reduction_indices=2)\n",
    "        mask = tf.sign(tf.reduce_max(tf.abs(target), reduction_indices=2))\n",
    "        cross_entropy *= mask\n",
    "        # Average over actual sequence lengths.\n",
    "        cross_entropy = tf.reduce_sum(cross_entropy, reduction_indices=1)\n",
    "        cross_entropy /= tf.reduce_sum(mask, reduction_indices=1)\n",
    "        return tf.reduce_mean(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load.py\n",
    "\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "import cPickle\n",
    "import random\n",
    "def atisfold():\n",
    "    f = open('data/data_set.pkl')\n",
    "    train_set, test_set, dicts = cPickle.load(f)\n",
    "    embedding = cPickle.load(open('data/embedding.pkl'))\n",
    "    return train_set, test_set,dicts,embedding\n",
    "\n",
    "def pad_sentences(sentences, padding_word=0, forced_sequence_length=None):\n",
    "    if forced_sequence_length is None:\n",
    "        sequence_length=max(len(x) for x in sentences)\n",
    "    else:\n",
    "        sequence_length=forced_sequence_length\n",
    "    padded_sentences=[]\n",
    "    for i in xrange(len(sentences)):\n",
    "        sentence=sentences[i]\n",
    "        num_padding=sequence_length-len(sentence)\n",
    "        if num_padding<0:\n",
    "            padded_sentence=sentence[0:sequence_length]\n",
    "        else:\n",
    "            padded_sentence=sentence+[int(padding_word)]*num_padding\n",
    "\n",
    "        padded_sentences.append(padded_sentence)\n",
    "\n",
    "    return padded_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#main.py\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "import tensorflow as tf\n",
    "import tensorlayer as tl\n",
    "import numpy as np\n",
    "import time\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "#import load\n",
    "import models.model as model\n",
    "import tools\n",
    "import sys\n",
    "\n",
    "def main():\n",
    "    s={\n",
    "        'nh1':300,\n",
    "        'nh2':300,\n",
    "        'win':3,\n",
    "        'emb_dimension':300,\n",
    "        'lr':0.1,\n",
    "        'lr_decay':0.5,\n",
    "        'max_grad_norm':5,\n",
    "        'seed':345,\n",
    "        'nepochs':150,\n",
    "        'batch_size':16,\n",
    "        'keep_prob':0.5,\n",
    "        'check_dir':'./checkpoints',\n",
    "        'display_test_per':3,\n",
    "        'lr_decay_per':10\n",
    "    }\n",
    "\n",
    "    train_set,test_set,dic,embedding=load.atisfold()\n",
    "    \n",
    "    \n",
    "    idx2label = dict((k,v) for v,k in dic['labels2idx'].iteritems())\n",
    "    idx2word  = dict((k,v) for v,k in dic['words2idx'].iteritems())\n",
    "\n",
    "    train_lex, train_y, train_z = train_set\n",
    "\n",
    "    tr = int(len(train_lex)*0.9)\n",
    "    valid_lex, valid_y, valid_z = train_lex[tr:], train_y[tr:], train_z[tr:]\n",
    "    train_lex, train_y, train_z = train_lex[:tr], train_y[:tr], train_z[:tr]\n",
    "    test_lex,  test_y, test_z  = test_set\n",
    "\n",
    "    print 'len(train_data) {}'.format(len(train_lex))\n",
    "    print 'len(valid_data) {}'.format(len(valid_lex))\n",
    "    print 'len(test_data) {}'.format(len(test_lex))\n",
    "\n",
    "    vocab = set(dic['words2idx'].keys())\n",
    "    vocsize = len(vocab)\n",
    "    print 'len(vocab) {}'.format(vocsize)\n",
    "    print \"Train started!\"\n",
    "\n",
    "    y_nclasses = 2\n",
    "    z_nclasses = 5\n",
    "\n",
    "    nsentences = len(train_lex)\n",
    "\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        rnn=model.Model(\n",
    "            nh1=s['nh1'],\n",
    "            nh2=s['nh2'],\n",
    "            ny=y_nclasses,\n",
    "            nz=z_nclasses,\n",
    "            de=s['emb_dimension'],\n",
    "            cs=s['win'],\n",
    "            lr=s['lr'],\n",
    "            lr_decay=s['lr_decay'],\n",
    "            embedding=embedding,\n",
    "            max_gradient_norm=s['max_grad_norm'],\n",
    "            model_cell='lstm'\n",
    "        )\n",
    "\n",
    "        checkpoint_dir=s['check_dir']\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.mkdir(checkpoint_dir)\n",
    "        checkpoint_prefix=os.path.join(checkpoint_dir,'model')\n",
    "\n",
    "        def train_step(cwords,label_y,label_z):\n",
    "            feed={\n",
    "                rnn.input_x:cwords,\n",
    "                rnn.input_y:label_y,\n",
    "                rnn.input_z:label_z,\n",
    "                rnn.keep_prob:s['keep_prob'],\n",
    "                rnn.batch_size:s['batch_size']\n",
    "            }\n",
    "            fetches=[rnn.loss,rnn.train_op]\n",
    "            loss,_=sess.run(fetches=fetches,feed_dict=feed)\n",
    "            return loss\n",
    "\n",
    "        def dev_step(cwords):\n",
    "            feed={\n",
    "                rnn.input_x:cwords,\n",
    "                rnn.keep_prob:1.0,\n",
    "                rnn.batch_size:s['batch_size']\n",
    "            }\n",
    "            fetches=rnn.sz_pred\n",
    "            sz_pred=sess.run(fetches=fetches,feed_dict=feed)\n",
    "            return sz_pred\n",
    "\n",
    "        saver=tf.train.Saver(tf.all_variables())\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "\n",
    "        best_f=-1\n",
    "        best_e=0\n",
    "        test_best_f=-1\n",
    "        test_best_e=0\n",
    "        best_res=None\n",
    "        test_best_res=None\n",
    "        for e in xrange(s['nepochs']):\n",
    "            tools.shuffle([train_lex,train_y,train_z],s['seed'])\n",
    "            t_start=time.time()\n",
    "            for step,batch in enumerate(tl.iterate.minibatches(train_lex,zip(train_y,train_z),batch_size=s['batch_size'])):\n",
    "                input_x,target=batch\n",
    "                label_y,label_z=zip(*target)\n",
    "                input_x=load.pad_sentences(input_x)\n",
    "                label_y=load.pad_sentences(label_y)\n",
    "                label_z=load.pad_sentences(label_z)\n",
    "                cwords=tools.contextwin_2(input_x,s['win'])\n",
    "                loss=train_step(cwords,label_y,label_z)\n",
    "\n",
    "                print 'loss %.2f' % loss,' [learning] epoch %i>> %2.2f%%' % (e,s['batch_size']*step*100./nsentences),'completed in %.2f (sec) <<\\r' % (time.time()-t_start),\n",
    "\n",
    "                sys.stdout.flush()\n",
    "\n",
    "            #VALID\n",
    "\n",
    "            predictions_valid=[]\n",
    "            predictions_test=[]\n",
    "            groundtruth_valid=[]\n",
    "            groundtruth_test=[]\n",
    "            for batch in  tl.iterate.minibatches(valid_lex,valid_z,batch_size=s['batch_size']):\n",
    "                x,z=batch\n",
    "                x=load.pad_sentences(x)\n",
    "                x=tools.contextwin_2(x,s['win'])\n",
    "                predictions_valid.extend(dev_step(x))\n",
    "                groundtruth_valid.extend(z)\n",
    "\n",
    "            res_valid=tools.conlleval(predictions_valid,groundtruth_valid,'')\n",
    "\n",
    "            if res_valid['f']>best_f:\n",
    "                best_f=res_valid['f']\n",
    "                best_e=e\n",
    "                best_res=res_valid\n",
    "                print '\\nVALID new best:',res_valid\n",
    "                path = saver.save(sess=sess, save_path=checkpoint_prefix, global_step=e)\n",
    "                print \"Save model checkpoint to {}\".format(path)\n",
    "            else:\n",
    "                print '\\nVALID new curr:',res_valid\n",
    "\n",
    "            #TEST\n",
    "            if e%s['display_test_per']==0:\n",
    "                for batch in tl.iterate.minibatches(test_lex, test_z, batch_size=s['batch_size']):\n",
    "                    x,z = batch\n",
    "                    x = load.pad_sentences(x)\n",
    "                    x = tools.contextwin_2(x, s['win'])\n",
    "                    predictions_test.extend(dev_step(x))\n",
    "                    groundtruth_test.extend(z)\n",
    "\n",
    "\n",
    "                res_test = tools.conlleval(predictions_test, groundtruth_test, '')\n",
    "\n",
    "                if res_test['f'] > test_best_f:\n",
    "                    test_best_f = res_test['f']\n",
    "                    test_best_e=e\n",
    "                    test_best_res=res_test\n",
    "                    print 'TEST new best:',res_test\n",
    "                else:\n",
    "                    print 'TEST new curr:',res_test\n",
    "\n",
    "            # learning rate decay if no improvement in 10 epochs\n",
    "            if e-best_e>s['lr_decay_per']:\n",
    "                sess.run(fetches=rnn.learning_rate_decay_op)\n",
    "            lr=sess.run(fetches=rnn.lr)\n",
    "            print 'learning rate:%f' % lr\n",
    "            if lr<1e-5:break\n",
    "            print\n",
    "\n",
    "        print \"Train finished!\"\n",
    "        print 'Valid Best Result: epoch %d:  ' % (best_e),best_res\n",
    "        print 'Test Best Result: epoch %d:  ' %(test_best_e),test_best_res\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict.py\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "import tensorflow as tf\n",
    "import tensorlayer as tl\n",
    "import numpy as np\n",
    "import time\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "import load\n",
    "import models.model as model\n",
    "\n",
    "import tools\n",
    "import sys\n",
    "\n",
    "def main():\n",
    "    s={\n",
    "        'nh1':300,\n",
    "        'nh2':300,\n",
    "        'win':3,\n",
    "        'emb_dimension':300,\n",
    "        'lr':0.1,\n",
    "        'lr_decay':0.5,\n",
    "        'max_grad_norm':5,\n",
    "        'seed':345,\n",
    "        'nepochs':50,\n",
    "        'batch_size':16,\n",
    "        'keep_prob':1.0,\n",
    "        'check_dir':'./checkpoints',\n",
    "        'display_test_per':5,\n",
    "        'lr_decay_per':10\n",
    "    }\n",
    "\n",
    "    \n",
    "    # load the dataset\n",
    "    train_set,test_set,dic,embedding=load.atisfold()\n",
    "    idx2label = dict((k,v) for v,k in dic['labels2idx'].iteritems())\n",
    "    idx2word  = dict((k,v) for v,k in dic['words2idx'].iteritems())\n",
    "\n",
    "    vocab = set(dic['words2idx'].keys())\n",
    "    vocsize = len(vocab)\n",
    "\n",
    "    test_lex,  test_y, test_z  = test_set[0:1000]\n",
    "\n",
    "    y_nclasses = 2\n",
    "    z_nclasses = 5\n",
    "\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        rnn = model.Model(\n",
    "            nh1=s['nh1'],\n",
    "            nh2=s['nh2'],\n",
    "            ny=y_nclasses,\n",
    "            nz=z_nclasses,\n",
    "            de=s['emb_dimension'],\n",
    "            cs=s['win'],\n",
    "            lr=s['lr'],\n",
    "            lr_decay=s['lr_decay'],\n",
    "            embedding=embedding,\n",
    "            max_gradient_norm=s['max_grad_norm'],\n",
    "            model_cell='lstm'\n",
    "        )\n",
    "\n",
    "        checkpoint_dir = s['check_dir']\n",
    "        saver = tf.train.Saver(tf.all_variables())\n",
    "        ckpt = tf.train.get_checkpoint_state(checkpoint_dir)\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "\n",
    "        def dev_step(cwords):\n",
    "            feed={\n",
    "                rnn.input_x:cwords,\n",
    "                rnn.keep_prob:1.0,\n",
    "                rnn.batch_size:s['batch_size']\n",
    "            }\n",
    "            fetches=rnn.sz_pred\n",
    "            sz_pred=sess.run(fetches=fetches,feed_dict=feed)\n",
    "            return sz_pred\n",
    "        print \"Test Results : \"\n",
    "        predictions_test=[]\n",
    "        groundtruth_test=[]\n",
    "        for batch in tl.iterate.minibatches(test_lex, test_z, batch_size=s['batch_size']):\n",
    "            x, z = batch\n",
    "            x = load.pad_sentences(x)\n",
    "            x = tools.contextwin_2(x, s['win'])\n",
    "            predictions_test.extend(dev_step(x))\n",
    "            groundtruth_test.extend(z)\n",
    "\n",
    "        res_test = tools.conlleval(predictions_test, groundtruth_test, '')\n",
    "\n",
    "        print res_test\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tools.py\n",
    "\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "import random\n",
    "\n",
    "def shuffle(lol,seed):\n",
    "    '''\n",
    "    lol :: list of list as input\n",
    "    seed :: seed the shuffling\n",
    "    shuffle inplace each list in the same order\n",
    "    '''\n",
    "    for l in lol:\n",
    "        random.seed(seed)\n",
    "        random.shuffle(l)\n",
    "\n",
    "def contextwin(l, win):\n",
    "    '''\n",
    "    win :: int corresponding to the size of the window\n",
    "    given a list of indexes composing a sentence\n",
    "    it will return a list of list of indexes corresponding\n",
    "    to context windows surrounding each word in the sentence\n",
    "    '''\n",
    "    assert (win % 2) == 1\n",
    "    assert win >=1\n",
    "    l = list(l)\n",
    "\n",
    "    lpadded = win/2 * [0] + l + win/2 * [0]\n",
    "    out = [ lpadded[i:i+win] for i in range(len(l)) ]\n",
    "\n",
    "    assert len(out) == len(l)\n",
    "    return out\n",
    "\n",
    "def contextwin_2(ls,win):\n",
    "    assert (win % 2) == 1\n",
    "    assert win >=1\n",
    "    outs=[]\n",
    "    for l in ls:\n",
    "        outs.append(contextwin(l,win))\n",
    "    return outs\n",
    "\n",
    "def getKeyphraseList(l):\n",
    "    res, now= [], []\n",
    "    for i in xrange(len(l)):\n",
    "        if l[i] != 0:\n",
    "            now.append(str(i))\n",
    "        if l[i] == 0 or i == len(l) - 1:\n",
    "            if len(now) != 0:\n",
    "                res.append(' '.join(now))\n",
    "            now = []\n",
    "    return set(res)\n",
    "\n",
    "def conlleval(predictions, groundtruth, file):\n",
    "    assert len(predictions) == len(groundtruth)\n",
    "    res = {}\n",
    "    all_cnt, good_cnt = len(predictions), 0\n",
    "    p_cnt, r_cnt, pr_cnt = 0, 0, 0\n",
    "    for i in range(all_cnt):\n",
    "        # print i\n",
    "        if all(predictions[i][0:len(groundtruth[i])] == groundtruth[i]) == True:\n",
    "            good_cnt += 1\n",
    "        pKeyphraseList = getKeyphraseList(predictions[i][0:len(groundtruth[i])])\n",
    "        gKeyphraseList = getKeyphraseList(groundtruth[i])\n",
    "        p_cnt += len(pKeyphraseList)\n",
    "        r_cnt += len(gKeyphraseList)\n",
    "        pr_cnt += len(pKeyphraseList & gKeyphraseList)\n",
    "    res['a'] = 1.0*good_cnt/all_cnt\n",
    "    res['p'] = 1.0*good_cnt/p_cnt\n",
    "    res['r'] = 1.0*good_cnt/r_cnt\n",
    "    res['f'] = 2.0*res['p']*res['r']/(res['p']+res['r'])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
